{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton’s method and nonlinear equations\n",
    "\n",
    "In first-year calculus, most students learn [Newton’s method](https://en.wikipedia.org/wiki/Newton's_method) for solving nonlinear equations $f(x) = 0$, which iteratively improves a sequence of guesses for the solution $x$ by **approximating f by a straight line**.   That is, it **approximates a *nonlinear* equation by a sequence of approximate *linear* equations**.\n",
    "\n",
    "This can be extended to *systems* of nonlinear equations as a **multidimensional Newton** method, in which we iterate by solving a sequence of linear (*matrix*) systems of equations.  This is one example of an amazing fact: **linear algebra is a fundamental tool even for solving nonlinear equations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pkg.add.([\"Interact\", \"PyPlot\", \"ForwardDiff\"]) # uncomment this line to install packages\n",
    "using Interact, PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-dimensional Newton\n",
    "\n",
    "The standard one-dimensional Newton’s method proceeds as follows.  Suppose we are solving for a zero (root) of $f(x)$:\n",
    "\n",
    "$$\n",
    "f(x) = 0\n",
    "$$\n",
    "\n",
    "for an arbitrary (but differentiable) function $f$, and we have a guess $x$.   We find an *improved* guess $x+\\delta$ by [Taylor expanding](https://en.wikipedia.org/wiki/Taylor_series) $f(x+\\delta)$ around $x$ to *first order* (linear!) in $\\delta$, and finding the .  (This should be accurate if $x$ is *close enough* to a solution, so that the $\\delta$ is *small*.)  That is, we solve:\n",
    "\n",
    "$$\n",
    "f(x + \\delta) \\approx f(x) + f'(x) \\delta = 0\n",
    "$$\n",
    "\n",
    "to obtain $\\delta = -f(x) / f'(x)$.  Plugging this into $x+\\delta$, we obtain:\n",
    "\n",
    "$$\n",
    "\\boxed{\\mbox{new } x = x - f(x)/f'(x)}.\n",
    "$$\n",
    "\n",
    "This is called a **Newton step**.  Then we simply repeat the process.\n",
    "\n",
    "Let's visualize this process for finding a root of $f(x) = 2\\cos(x) - x + x^2/10$ (a [transcendental equation](https://en.wikipedia.org/wiki/Transcendental_equation) that has no closed-form solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure()\n",
    "xs = linspace(-5,5,1000)\n",
    "@manipulate for step in slider(1:20, value=1), start in slider(-4:0.1:4, value=-0.1)\n",
    "    withfig(fig) do\n",
    "        x = start\n",
    "        local xprev, f, f′\n",
    "        for i = 1:step\n",
    "            xprev = x\n",
    "            f = 2cos(x) - x + x^2/10\n",
    "            f′ = -2sin(x) - 1 + 2x/10\n",
    "            x = x - f/f′\n",
    "        end\n",
    "        plot(xs, 0*xs, \"k-\")\n",
    "        plot(xs, 2cos.(xs) - xs + xs.^2/10, \"b-\")\n",
    "        newf = 2cos(x) - x + x^2/10\n",
    "        plot([xprev, x], [f, 0.0], \"ro\")\n",
    "        plot(x, newf, \"bo\")\n",
    "        plot(xs, f + f′ * (xs - xprev), \"r--\")\n",
    "        plot([x, x], [0, newf], \"k--\")\n",
    "        xlim(minimum(xs), maximum(xs))\n",
    "        ylim(-5,4)\n",
    "        title(\"Newton step $step: f($x) = $newf\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start it anywhere near a root of $f(x)$, Newton’s method can converge extremely quickly: **asymptotically, it doubles the number of accurate digits on each step**.\n",
    "\n",
    "However, if you start it far from a root, the convergence can be hard to predict, and it may not even converge at all (it can oscillate forever around a local minimum).\n",
    "\n",
    "Still, in many practical applications, there are ways to get a good initial guess for Newton, and then it is an extremely effective way to solve nonlinear equations to high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Multidimensional Newton\n",
    "\n",
    "The general case of the multidimensional Newton algorithm is as follows.  We are solving:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "f_1(x_1, \\ldots, x_n) \\\\\n",
    "f_2(x_1, \\ldots, x_n) \\\\\n",
    "\\vdots \\\\\n",
    "f_n(x_1, \\ldots, x_n)\n",
    "\\end{pmatrix}\n",
    "= f(x) = 0\n",
    "$$\n",
    "\n",
    "for $x \\in \\mathbb{R}^n$ and $f(x) \\in \\mathbb{R}^n$: $n$ (possibly nonlinear but differentiable) equations in $n$ unknowns.\n",
    "\n",
    "Given a guess $x$, we want to find an improved guess $x + \\delta$ for $\\delta \\in \\mathbb{R}^n$.   We do this by Taylor-expanding $f$ around $x$ to first order (linear):\n",
    "\n",
    "$$\n",
    "f(x + \\delta) \\approx f(x) + J(x) \\, \\delta  = 0\n",
    "$$\n",
    "\n",
    "where $J$ is the $n \\times n$ Jacobian matrix with entries $\\boxed{ J_{ij} = \\partial f_i / \\partial x_j }$, i.e.\n",
    "\n",
    "$$\n",
    "J = \\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\ddots\n",
    "\\end{pmatrix} \\; .\n",
    "$$\n",
    "\n",
    "Hence, we solve the *linear* equation\n",
    "\n",
    "$$\n",
    "J(x) \\, \\delta = -f(x)\n",
    "$$\n",
    "\n",
    "for the Newton step $\\delta$, obtaining (if $J$ is invertible):\n",
    "\n",
    "$$\n",
    "\\boxed{ x + \\delta = x - J(x)^{-1} f(x) }\n",
    "$$\n",
    "\n",
    "Some things to remember:\n",
    "\n",
    "* Newton converts $n$ *nonlinear* equations into repeated solution of $n \\times n$ *linear* equations.\n",
    "\n",
    "* When solving nonlinear equations, coming up with a good initial guess is a bit of an art, that often requires some problem-specific understanding.  A typical trick is to solve a related problem, e.g. a linear problem.  Or to start with a linear problem and to \"turn on\" the nonlinearity gradually, using the solution of each nonlinear problem as the starting guess for the next one.\n",
    "\n",
    "* If you start too far from a root, Newton’s method can sometimes take a large step, far outside the validity of the Taylor approximation, and this can actually make the guess *worse*.  Sophisticated implementations use a variety of techniques to make the convergence more robust, such as a [backtracking line search](https://en.wikipedia.org/wiki/Backtracking_line_search) or a [trust region](https://en.wikipedia.org/wiki/Trust_region).  These techniques are outside the scope of 18.06, though!\n",
    "\n",
    "* There are other methods related to Newton that don't require you to compute $J(x)$ at all.  You only supply $f(x)$ and they either approximate the Jacobian for you directly (e.g. [Broyden’s method](https://en.wikipedia.org/wiki/Broyden's_method)) or implicitly (e.g. [Anderson acceleration](http://epubs.siam.org/doi/abs/10.1137/10078356X)).  There is a rich mathematical literature on solution methods for nonlinear systems of equations, but essentially all the methods have one thing in common: *linear* algebra plays a key role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "Computing the Jacobian, while in principle is a simple 18.02 exercise, is often tedious and error prone for large systems of equations.\n",
    "\n",
    "Instead, the Jacobian matrix can often be *automatically* computed from $f(x)$ by the computer using [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) tools, saving you from the tedious (and error-prone) task of writing out $J(x)$ manually.  (How this works, involving [dual numbers](https://en.wikipedia.org/wiki/Dual_number), is [really cool](https://arxiv.org/abs/1607.07892), but somewhat outside the scope of 18.06.)\n",
    "\n",
    "In Julia, there are packages [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl) and [ReverseDiff](https://github.com/JuliaDiff/ReverseDiff.jl) to do this for you.\n",
    "\n",
    "For example, let's compute the Jacobian for $$f_\\mathrm{ex}(x) = \\begin{pmatrix} \\sin(x_1 x_2) - 0.5 \\\\ x_1^2 - x_2^2 \\end{pmatrix},$$ which should give $$J_\\mathrm{ex}(x) = \\begin{pmatrix} x_2 \\cos(x_1 x_2) & x_1 \\cos(x_1 x_2) \\\\ 2x_1 & -2x_2 \\end{pmatrix}:$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fₑₓ(x) = [ sin(x[1]*x[2]) - 0.5\n",
    "        x[1]^2 - x[2]^2 ]\n",
    "\n",
    "# manual Jacobian\n",
    "Jₑₓ(x) = [ x[2]*cos(x[1]*x[2])  x[1]*cos(x[1]*x[2])\n",
    "         2x[1]               -2x[2]               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fₑₓ([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jₑₓ([2,3]) # manual Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.jacobian(fₑₓ, [2,3]) # automatic Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.jacobian(fₑₓ, [2,3]) - Jₑₓ([2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional root finding example\n",
    "\n",
    "As a simple example, let's find a root of our test function\n",
    "$$f_\\mathrm{ex}(x) = \\begin{pmatrix} \\sin(x_1 x_2) - 0.5 \\\\ x_1^2 - x_2^2 \\end{pmatrix}$$\n",
    "from above.\n",
    "\n",
    "We can actually solve this analytically. $f_\\mathrm{ex}(x) = 0$ immediately tells us $x_1 = \\pm x_2$ from $x_1^2 - x_2^2 = 0$.  If we let $x = x_1 = x_2$, then $\\sin(x^2) - 0.5$ has its first root at $x = \\sqrt{\\pi/6}$.   We can also see these roots graphically by plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(10,4))\n",
    "\n",
    "subplot(1,2,1)\n",
    "X = linspace(-3,3,250)\n",
    "pcolor(X', X, [-log(0.01 + norm(fₑₓ([x,y]))) for y in X, x in X], cmap=\"gray\")\n",
    "xlabel(L\"x_1\")\n",
    "ylabel(L\"x_2\")\n",
    "title(L\"-\\log [ 0.01 + \\Vert f_\\mathrm{ex}(x) \\Vert]\")\n",
    "colorbar()\n",
    "\n",
    "subplot(1,2,2)\n",
    "x = linspace(0,3,200)\n",
    "plot(x, sin.(x.^2) .- 0.5, \"r-\")\n",
    "plot(x, 0*x, \"k-\")\n",
    "plot([sqrt(pi/6), sqrt(5pi/6), sqrt(13pi/6)], [0,0,0], \"ro\", markersize=8)\n",
    "text(0.65, -0.23, L\"\\sqrt{\\pi/6}\")\n",
    "title(L\"\\sin(x^2) - \\frac{1}{2}\")\n",
    "xlabel(L\"x = x_1 = x_2\")\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the coordinate of our first root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(π/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the progress after varying numbers of Newton steps starting at a crude initial guess $x = \\begin{pmatrix} 0.5 \\\\ 0.8 \\end{pmatrix}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.5, 0.8]\n",
    "for i = 1:10\n",
    "    x = x - Jₑₓ(x) \\ fₑₓ(x)  # Newton step\n",
    "    println(\"$i Newton steps: x = $x, f(x) = \", fₑₓ(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, super fast convergence: **15 decimal places in only four steps!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A nonlinear circuit problem\n",
    "\n",
    "Consider the nonlinear circuit graph from the [graphs and networks lecture](http://nbviewer.jupyter.org/github/stevengj/1806-spring17/blob/master/lectures/Graphs-Networks.ipynb):\n",
    "\n",
    "<img src=\"https://github.com/stevengj/1806-spring17/raw/master/lectures/circuit.png\" width=\"300\">\n",
    "\n",
    "The incidence matrix $A$ of this graph is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [ -1   0   0   1   0   0\n",
    "       0   0   0  -1   1   0\n",
    "       0   0   0   0  -1   1\n",
    "       0   0   1   0   0  -1\n",
    "       0   1  -1   0   0   0\n",
    "       1  -1   0   0   0   0\n",
    "       0  -1   0   0   0   1\n",
    "       0   0   0  -1   0   1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: (Linear) circuit equations\n",
    "\n",
    "Recall that if we associate a vector $v$ of voltages with the 6 nodes, then $d = Av$ gives the **voltage rise** across each edge, and $i = -YAv$ gives the **current** through each edge, where $Y$ is a diagonal matrix of admittances (= 1/resistance)\n",
    "\n",
    "$$\n",
    "Y = \\begin{pmatrix} Y_1 &     &        &     \\\\\n",
    "                        & Y_2 &        &     \\\\\n",
    "                        &     & \\ddots &     \\\\\n",
    "                        &     &        & Y_8 \\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This is simply an expression of Ohm's law.\n",
    "\n",
    "Furthermore, we showed that Kirchhoff’s current law is just the statement $A^T i = 0$.   Putting it all together, and including a current source term $s$ (an external current flowing out of each node), we obtained the equations:\n",
    "\n",
    "$$\n",
    "A^T Y A v = s\n",
    "$$\n",
    "\n",
    "where to have a solution ($A^T Y A$ is singular) we had to have $s \\perp N(A)$, or equivalently $\\sum_i s_i = 0$: the net current flowing in/out of the circuit must be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Ohm’s law\n",
    "\n",
    "A key physical foundation of the equations above was Ohm’s law: $i_j = -d_j/R_j = - Y_j d_j$, which is the statement that the **current is proportional to the voltage drop** $-d_j$.\n",
    "\n",
    "However, this is only an approximation.  In reality, as the voltage and current increase, the resistance changes.  For example, the resistor heats up (and eventually melts!) due to the dissipated power $i_j^2 / Y_j = Y_j d_j^2$, and resistance depends on temperature.\n",
    "\n",
    "Let's try a **simple model** of a voltage-dependent resistance.  Suppose that we modify Ohm’s law to:\n",
    "\n",
    "$$\n",
    "i_j = - \\underbrace{ \\frac{Y_j }{1 + \\alpha_j d_j^2} }_{\\tilde{Y}_j(d_j)} d_j\n",
    "$$\n",
    "\n",
    "where $\\tilde{Y}_j(d_j) = Y_j / (1 + \\alpha_j d_j^2)$ corresponds to a resistance that increases quadratically with the voltage rise $d_j$.  This model is not unrealistic!   For a real resistor, you could measure the voltage dependence of $Y$, and *fit* it to an equation of this form, and it would be valid for sufficiently small $d_j$!  (The admittance will normally only depend on $d^2$, not on $d$, because with most materials it will not depend on the sign of the voltage drop or current.)\n",
    "\n",
    "The problem, of course, is that with this nonlinear Ohm’s law, the whole problem becomes a **nonlinear system of equations**.  How can we solve such a system of equations?  At first glance, the methods of 18.06 don't work — they are only for *linear* systems.\n",
    "\n",
    "But, we can apply Newton's method as described above, with automatic differentiation via `ForwardDiff`, to solve this by a sequence of linear problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "For an example, let's just set $Y_k = 1$ and $\\alpha_k = 0.5$ for $k=1,\\ldots,6$, and use $s = (1,-1,0,0,0,0)$ as in the previous lecture (current injected into node 2 and extracted out from node 1).\n",
    "\n",
    "What should we use as our initial guess?  How about the solution to the *linear* problem with $\\tilde{Y}(0) = Y$?  That's often a good guess, since in many real problems the nonlinear solution will be very close to the solution of a simplified linear problem.\n",
    "\n",
    "Let's write some code to compute $f(v)$ and $J(v)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yₖ = 1\n",
    "αₖ = 0.5\n",
    "Ỹₖ(d) = Yₖ / (1 + αₖ * d^2)\n",
    "\n",
    "f(v) = A' * (diagm(Ỹₖ.(A*v)) * (A*v)) - [1,-1,0,0,0,0]\n",
    "J(v) = ForwardDiff.jacobian(f, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the Newton step $v - J(v)^{-1} f(v)$.\n",
    "\n",
    "I'd ideally like to use `v - J(v) \\ f(v)` in Julia,\n",
    "but the ``` \\ ``` function will complain that `J` is singular, even though `f` is in the column space as discussed below.  There are various ways to do this properly numerically, but for simplicity I will \"cheat\" and use and advanced tool called the [pseudo-inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse), computed in julia by `pinv(J)`, which (for a right-hand-side in the column space) will give us a particular solution similar to what we would get from the elimination technique we learned in class.   (We will cover the pseudo-inverse much later in 18.06.)  There are other ways to proceed here that are even more efficient, but `pinv` is the simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtonstep(v) = v - pinv(J(v)) * f(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what happens.  We'll start by just plotting the voltages as a function of the Newton step, to see how (and whether) it is converging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton(v, nsteps)\n",
    "    for i = 1:nsteps\n",
    "        v = newtonstep(v)\n",
    "    end\n",
    "    return v\n",
    "end\n",
    "newton(v, nsteps::AbstractVector) = map(n -> newton(v,n), nsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M₀ = A' * diagm(Ỹₖ.(zeros(8))) * A # matrix AᵀYA for zero voltage drops (i.e. linear problem)\n",
    "v₀ = pinv(M₀) * [1,-1,0,0,0,0]  # initial guess = solution to linear problem\n",
    "vsteps = newton(v₀, 0:10)\n",
    "plot([vsteps[i][j] for i=1:length(vsteps), j=1:6], \"o-\")\n",
    "xlabel(\"Newton steps\")\n",
    "ylabel(\"node voltages\")\n",
    "legend([L\"v_1\", L\"v_2\", L\"v_3\", L\"v_4\", L\"v_5\", L\"v_6\"], loc=\"upper right\")\n",
    "title(L\"Newton steps for $\\alpha = 0.5$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, it is converging pretty rapidly.  Another way to see this is to plot the convergence of the length (norm) of the $f(v)$ vector, $\\Vert f(v) \\Vert = \\sqrt{f(v)^T f(v)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel(\"Newton steps\")\n",
    "ylabel(L\"\\Vert f(v) \\Vert\")\n",
    "semilogy(norm.(f.(vsteps)), \"bo-\")\n",
    "title(L\"Newton convergence for $\\alpha = 0.5$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It converges **faster than exponentially** with the step.  Once it is close to the solution, Newton roughly **doubles the number of digits in each step**.\n",
    "\n",
    "Eventually, it stops getting better: the accuracy is **limited by roundoff errors** once the error reaches $\\approx 10^{-16}$, related to the fact that the computer is only doing arithmetic to about 16 digits of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Jacobian manually\n",
    "\n",
    "It might be nice to look more explicitly at the Jacobian matrix to understand why it is singular in this problem, and why that is okay here (i.e. why a solution exists).  In really large computational problems, people often compute Jacobians more explicitly in order to extract computational savings (by exploiting any special forms of the matrices in the analytical result).\n",
    "\n",
    "It is also, unfortunatey, a lot messier and more error prone to compute the Jacobian manually.  I often like to think of the Jacobian in terms of Taylor expanding: if you **Taylor expand everything to first order**, throwing away higher-order terms, then the **Jacobian matrix is the coefficient of the 1st-order term** in the final result.\n",
    "\n",
    "The admittance is written in terms of the voltage rise $d = Av$.  If we change $v$ to $v+\\delta$, then we get $d + A\\delta$.  Let's denote $\\hat{\\delta} = A\\delta$.  Then the formula for each component of our current $i$ becomes:\n",
    "\n",
    "$$\n",
    "i_j = -\\tilde{Y}_j(d_j +\\hat{\\delta}_j) \\, (d_j +\\hat{\\delta}_j) \\approx -\\tilde{Y}_j(d_j) d_j - (\\tilde{Y}_j(d_j) + \\tilde{Y}_j'(d_j) d_j) \\hat{\\delta}_j\n",
    "$$\n",
    "\n",
    "where we have just Taylor-expanded around $\\hat{\\delta}_j = 0$, and $\\tilde{Y}_j'(d_j) = -2 \\alpha_j d_j Y_j / (1 + \\alpha_j d_j^2)^2$.  Let $K_j(d_j) = \\tilde{Y}_j(d_j) + \\tilde{Y}_j'(d_j) d_j$, and then we have:\n",
    "\n",
    "$$\n",
    "i_j \\approx - \\left[ \\tilde{Y}_j(d_j) d_j + K_j(d_j) \\hat{\\delta}_j \\right]\n",
    "$$\n",
    "\n",
    "Plugging this into $f(v+\\delta)$, we get\n",
    "\n",
    "$$\n",
    "f(v+\\delta) \\approx \\underbrace{A^T Y(Av) Av - s}_{f(v)} + \\underbrace{A^T K(Av) A}_{J(v)} \\delta\n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "J(v) = A^T K(Av) A\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "K(d) = \\begin{pmatrix} K_1(d_1) &     &        &     \\\\\n",
    "                        & K_2(d_2) &        &     \\\\\n",
    "                        &     & \\ddots &     \\\\\n",
    "                        &     &        & K_8(d_8) \\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "There is one slight problem here: $J$ is a singular matrix even if all $K_j \\neq 0$, because of $N(A) \\ne \\{0\\}$.   Just as with $s$, $J(v) y = f(v)$ only has a solution if $f(v) \\in C(J) \\subseteq C(A^T)$.  Fortunately, $f(v) = A^T Y A - s$ is *always* in $C(A^T)$ as long as $s \\in C(A^T) = N(A)^\\perp$, which was required *anyway* (from above) if we are to have a solution in the *linear* case.\n",
    "\n",
    "(We can still run into a singular $J$ for \"unlucky\" $K_j$, but that is a typical hazard of Newton's method, just like we can run into $f'(x)=0$ for unlucky $x$ values.  The important thing is that it doesn't happen \"generically\", i.e. singularities only occur at isolated points.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ỹₖ′(d) = -2αₖ*d*Yₖ / (1 + αₖ * d^2)^2\n",
    "Kₖ(d) = Ỹₖ(d) + Ỹₖ′(d)*d\n",
    "J_manual(v) = A' * diagm(Kₖ.(A*v)) * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this against our automatic Jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [0.1,1.2,3.4,5.6,-0.3,0.7] # a \"random\" vector\n",
    "ForwardDiff.jacobian(f, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_manual(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.jacobian(f, v) - J_manual(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, it computed our Jacobian automatically and perfectly accurately (up to the machine-precision rounding errors).\n",
    "\n",
    "[How ForwardDiff works](https://arxiv.org/abs/1607.07892), based on [dual numbers](https://en.wikipedia.org/wiki/Dual_number), is outside the scope of 18.06, but there is a nice [tutorial notebook](http://nbviewer.jupyter.org/github/stevengj/18S096-iap17/blob/master/lecture8/Automatic%20differentiation%20and%20applications.ipynb) by [David Sanders](http://sistemas.fciencias.unam.mx/~dsanders/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  },
  "widgets": {
   "state": {
    "0256c79a-4ea4-4bc5-8716-6f0859951218": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "51575b3a-dace-4a85-aece-92e117ab0b40": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "a0358c2f-65a4-47b3-9058-fc314b5937ca": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    },
    "ab41f0e8-652b-49ec-8056-5a0b11e728d8": {
     "views": [
      {
       "cell_index": 4
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
